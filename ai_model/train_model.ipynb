{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "297a8081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—‚ï¸ Found 92 parquet files\n",
      "âœ… Combined 92 files â€” total samples: 91081\n",
      "Available columns: ['image', 'brand', 'model']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "# df = pd.read_parquet('dataset/dataset_batch_40.parquet')\n",
    "# print(f\"Total samples: {len(df)}\")\n",
    "# print(df.head())\n",
    "\n",
    "# path where your parquet files are stored\n",
    "data_path = \"datasets/\"   # e.g. ./datasets/ or /Users/you/projects/sneakers/datasets/\n",
    "files = glob.glob(os.path.join(data_path, \"*.parquet\"))\n",
    "\n",
    "print(f\"ðŸ—‚ï¸ Found {len(files)} parquet files\")\n",
    "\n",
    "# load all parquet files and concatenate into one big DataFrame\n",
    "dfs = [pd.read_parquet(f) for f in files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"âœ… Combined {len(files)} files â€” total samples: {len(df)}\")\n",
    "print(\"Available columns:\", list(df.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2bb1616e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined 92 files â€” total samples: 91081\n",
      "Available columns: ['image', 'brand', 'model']\n"
     ]
    }
   ],
   "source": [
    "# load all parquet files and concatenate into one big DataFrame\n",
    "dfs = [pd.read_parquet(f) for f in files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"âœ… Combined {len(files)} files â€” total samples: {len(df)}\")\n",
    "print(\"Available columns:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23686818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 brands:\n",
      "brand\n",
      "Nike        29999\n",
      "Adidas      20700\n",
      "Jordan       7800\n",
      "adidas       5600\n",
      "Asics        4000\n",
      "Vans         3685\n",
      "New          3500\n",
      "Converse     3101\n",
      "Reebok       3100\n",
      "Puma         2800\n",
      "Name: count, dtype: int64\n",
      "                                               image brand     model\n",
      "0  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...  Puma  RBD Game\n",
      "1  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...  Puma  RBD Game\n",
      "2  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...  Puma  RBD Game\n"
     ]
    }
   ],
   "source": [
    "# check how many unique brands exist\n",
    "print(\"\\nTop 10 brands:\")\n",
    "print(df['brand'].value_counts().head(10))\n",
    "\n",
    "# check a few rows\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ebf6278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaned dataset â€” remaining rows: 91081, brands: 40\n"
     ]
    }
   ],
   "source": [
    "# drop rows with missing image bytes or brand labels\n",
    "df = df.dropna(subset=['image', 'brand'])\n",
    "\n",
    "# remove brands with too few samples (less than 5)\n",
    "df = df.groupby('brand').filter(lambda x: len(x) > 5)\n",
    "\n",
    "print(f\"ðŸ§¹ Cleaned dataset â€” remaining rows: {len(df)}, brands: {df['brand'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c90fddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved combined dataset as: all_sneakers_combined.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save the combined dataset for future use\n",
    "output_path = \"all_sneakers_combined.parquet\"\n",
    "df.to_parquet(output_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved combined dataset as: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff156e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded dataset: 91081 rows, 40 brands\n"
     ]
    }
   ],
   "source": [
    "# Reload to verify\n",
    "df_check = pd.read_parquet(\"all_sneakers_combined.parquet\")\n",
    "print(f\"Reloaded dataset: {len(df_check)} rows, {df_check['brand'].nunique()} brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aac4d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 72864, Validation: 18217\n",
      "Brand Distribution:\n",
      "brand\n",
      "Nike           29999\n",
      "Adidas         20700\n",
      "Jordan          7800\n",
      "adidas          5600\n",
      "Asics           4000\n",
      "Vans            3685\n",
      "New             3500\n",
      "Converse        3101\n",
      "Reebok          3100\n",
      "Puma            2800\n",
      "Saucony         1100\n",
      "Timberland       700\n",
      "ON               600\n",
      "Off-White        500\n",
      "Autry            500\n",
      "Clarks           400\n",
      "Balenciaga       300\n",
      "Veja             300\n",
      "Salomon          200\n",
      "Diadora          200\n",
      "Mizuno           100\n",
      "Lacoste          100\n",
      "Alexander        100\n",
      "alexander        100\n",
      "Le               100\n",
      "Karhu            100\n",
      "Dr.              100\n",
      "Hoka             100\n",
      "Ewing            100\n",
      "Camper           100\n",
      "Suicoke          100\n",
      "Crocs            100\n",
      "Moon             100\n",
      "Keen             100\n",
      "Onitsuka         100\n",
      "Lanvin           100\n",
      "BAPE             100\n",
      "Amiri            100\n",
      "Birkenstock      100\n",
      "KangaROOS         96\n",
      "Name: count, dtype: int64\n",
      "Model Distribution:\n",
      "model\n",
      "100                                 2281\n",
      "Waffle One                           300\n",
      "Gel Respector                        300\n",
      "React Infinity Run Flyknit           300\n",
      "Air Vapormax Plus                    300\n",
      "                                    ... \n",
      "Authentic                             85\n",
      "Balance New Balance 580               85\n",
      "Gel BND                               45\n",
      "Balance New Balance 997               15\n",
      "Chuck Taylor All Star Top Canvas       1\n",
      "Name: count, Length: 770, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "\n",
    "df = pd.read_parquet(\"all_sneakers_combined.parquet\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['brand'], random_state=42)\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}\")\n",
    "print(f\"Brand Distribution:\\n{df['brand'].value_counts()}\")\n",
    "print(f\"Model Distribution:\\n{df['model'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f1c40d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class(JPEG Bytes â†’ Tensor)\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class SneakerArrayDataset(Dataset):\n",
    "    def __init__(self, df, label_col=\"brand\"):\n",
    "        self.images = df['image'].values\n",
    "        self.labels = df[label_col].astype('category').cat.codes\n",
    "        self.label2name = dict(enumerate(df[label_col].astype('category').cat.categories))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert binary bytes â†’ Image\n",
    "        img_bytes = self.images[idx]\n",
    "        \n",
    "        # Some Parquet exports store bytes as strings, ensure conversion\n",
    "        if isinstance(img_bytes, str):\n",
    "            img_bytes = bytes(img_bytes, 'utf-8')\n",
    "        \n",
    "        image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "        \n",
    "        # Resize + Normalize\n",
    "        image = image.resize((224, 224))\n",
    "        img_tensor = torch.tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        \n",
    "        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return img_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20ab93a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 40\n"
     ]
    }
   ],
   "source": [
    "# Data Loaders\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = SneakerArrayDataset(train_df)\n",
    "val_dataset = SneakerArrayDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Number of classes: {len(set(train_dataset.labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d2e4569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Test one batch\n",
    "sample_img, sample_label = train_dataset[0]\n",
    "print(sample_img.shape, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0575082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paabonsu/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/paabonsu/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded from local weights: /Users/paabonsu/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained Model (ResNet50)\n",
    "# -----------------------------\n",
    "# Manual download + load\n",
    "# -----------------------------\n",
    "import os\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "# path to the downloaded weights\n",
    "weights_path = os.path.expanduser(\"~/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\")\n",
    "\n",
    "# create model without attempting to download\n",
    "model = models.resnet50(pretrained=False)\n",
    "state = torch.load(weights_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ… Model loaded from local weights: {weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "61a23c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/0] | Loss: 0.6801\n",
      "Epoch [2/1] | Loss: 0.3357\n",
      "Epoch [3/2] | Loss: 0.2395\n",
      "Epoch [4/3] | Loss: 0.1822\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m imgs, labels = imgs.to(device), labels.to(device)\n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     16\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torchvision/models/resnet.py:274\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    271\u001b[39m x = \u001b[38;5;28mself\u001b[39m.maxpool(x)\n\u001b[32m    273\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer1(x)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer3(x)\n\u001b[32m    276\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer4(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torchvision/models/resnet.py:146\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    144\u001b[39m     identity = x\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.bn1(out)\n\u001b[32m    148\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.relu(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal/AI/sneaker-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Loss, Optimzer, and Training Loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 10\n",
    "\n",
    "for epochs in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epochs+1}/{epochs}] | Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0ada0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Partial model saved safely.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"checkpoint_manual_stop.pth\")\n",
    "print(\"ðŸ’¾ Partial model saved safely.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e8e1a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on Validation Set\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3afdfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sneaker: Adidas\n"
     ]
    }
   ],
   "source": [
    "# Test Single Prediction\n",
    "model.eval()\n",
    "test_img, _ = val_dataset[0]\n",
    "test_img = test_img.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_img)\n",
    "    _, pred = torch.max(output, 1)\n",
    "\n",
    "predicted_label = train_dataset.label2name[pred.item()]\n",
    "print(f\"Predicted Sneaker: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b25362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "for i in range(5):\n",
    "    img_tensor, label = val_dataset[i]\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor.unsqueeze(0).to(device))\n",
    "        _, pred = torch.max(output, 1)\n",
    "        predicted_label = train_dataset.label2name[pred.item()]\n",
    "        actual_label = train_dataset.label2name[label.item()]\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Predicted: {predicted_label} | Actual: {actual_label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'label_map': train_dataset.label2name\n",
    "}, \"sneaker_model.pth\")\n",
    "\n",
    "print(\"ðŸ’¾ Model and label map saved as sneaker_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
